This module is the backbone of our system. 
Its mission is to parse the raw csv files, clean the bugs, create a new multi-index data structure and finally output it into HDF buckets.

The code is fully documented so in this document we aim to discuss the main intuition behind it.
As we noted in another document, our trading application is time series driven. 
Therefore we would like our tick data to be indexed and structured based on their respective time stamp. 
For all the relational-db-techies this rings one bell -- primary index.
Apart from this very natural decision there are other matters to be concerned.
For every tick, we are usually going to have many features(like bid, offer, mid etc) 
and for each one of them many more attributes(as price, volume, slope, order list). 
We would like to have them calculated offline and saved to disk so we can access them without recomputing them every time.
The amount of data gets really nasty if we hypothesize even deeper levels of hierarchy
(as for example for every one of the features above, store an N-size depth book).
So we thought, that another secondary hierarchical index with multiple levels would be capable of dealing 
with the organization of the data and help a lot in the performance of the system.

The resulting data structure is a pandas data frame which has as main index the time stamps in the rows 
and a multi-level index, which organizes all the features in the columns.
Below you can see the very basic scheme of this structure so you can get an idea of what is going to be consisted of.

 								  A                                                                                       B                                                                                 
                                  p                                           v                                           p                                            v                                    
                                  1       2        3        4        5        1        2        3        4        5       1        2        3        4        5        1        2        3        4        5
2012-08-08 00:00:45.442000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  4180000  2000000  0.7932  0.79319  0.79318  0.79317  0.79316   500000  2000000  1000000  2500000  3500000
2012-08-08 00:00:45.641000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  4180000  2000000  0.7932  0.79319  0.79318  0.79317  0.79316   500000  2000000  1000000  2500000  3000000
2012-08-08 00:00:45.844000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  5180000  1000000  0.7932  0.79319  0.79318  0.79317  0.79316   500000  2000000  1000000  1500000  4000000
2012-08-08 00:00:47.268000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  5180000  1000000  0.7932  0.79319  0.79317  0.79316  0.79315   500000  2000000  2500000  4000000  1000000
2012-08-08 00:00:47.573000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  5180000  1000000  0.7932  0.79319  0.79318  0.79317  0.79316   500000  2000000  1000000  1500000  4000000
2012-08-08 00:00:48.287000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  5180000  1000000  0.7932  0.79319  0.79317  0.79316  0.79315   500000  2000000  2500000  4000000  1000000
2012-08-08 00:00:51.241000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  5180000  1000000  0.7932  0.79319  0.79318  0.79317  0.79316   500000  2000000  1000000  1500000  4000000
2012-08-08 00:00:51.651000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000  1500000   500000  4180000  2000000  0.7932  0.79319  0.79317  0.79316  0.79315   500000  3000000  2500000  3000000  1000000
2012-08-08 00:00:52.063000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000   500000  1500000  2180000  3000000  0.7932  0.79319  0.79317  0.79316  0.79315  1500000  2000000  4500000  1000000  1000000
2012-08-08 00:00:52.165000  0.79339  0.7934  0.79341  0.79343  0.79344  1000000   500000  1500000  2180000  4000000  0.7932  0.79319  0.79318  0.79317  0.79316  1500000  1000000  1000000  4500000  1000000

The above listens to the name CapitalKDF. 
You can easily manipulate the data by rows: capitalkdf.ix[55:242]
			            by columns: capitalkdf['A'] or capitalkdf.A
		             	            by different levels of the columns index: capitalkdf['A','p'] or capitalkdf['B','v','3']

In a following file we will demonstrate a demo of many operations you can perform on this data structure like
getting statistical summaries of the data, plotting many time series in the same time, finding correlations among currency pairs,
plotting auto-correlation, finding the maximum spread and many more, so stay tuned...

Another advantage of this structure is that it can dynamically alter its scheme and perform calculations and statistics 
on the fly in a very efficient way.
With just a simple command you can for example calculate the average of the 5 ask prices and store them in a new column in the end. 
This way you can dynamically add or even delete features that you don't need without having to create everything from scratch.
For example, capitalkdf['hello']= capitalkdf['B','v','3'].cumsum(), creates a new column with name "hello" that has as values,
the cumulative sum of the bid volumes at level 3 of the order book for every time frame.

One last thing to note is that we may come across two consecutive order books with the same time stamp while reading the csv files.
This usually occurs when there is only one message (i.e. an 'A') followed by the print of the order book.
Anyhow, we want to keep only the most recent valid form of the order book and this is exactly what we do, 
by overwriting a previous one in case of the same time stamp.

A few words about the performance of our code:
-----------------------------------------------
We tried several versions and different approaches to determine which one is more time efficient and thus should be adopted.
You can see a summary of each effort in commented lines, inside the code.
But in order to get an intuition on what is the main issue, just imagine what we are dealing with.
We must parse a file (which a-priori we don't know its size) and continuously append a data structure to it.

The naive way was to create an empty structure, parse the file and each time we needed to save an order book, append capitalkdf.
This approach was awful though as it took more than 40 minutes to finish one file
(I don't even know how much it would take as I got mad and stopped it).

The solution was only one: preallocation. 
Therefore, we parse the file once, find out how many order books are needed to be printed
(we take into account the RESTART messages so we decrease the number if needed) and we create a capitalkdf with size equal to 
the respective number of the order books about to be held. We fill all the positions with -1 so as to indicate an empty position
In cases where the depth of the order book isn't completely filled, a negative value will indicate that there is no record for this depth.

The second attempt was to keep an order book instance in memory and mess with it every time we want to add, modify or delete
prices and volumes and once we were ready to output the state of the order book, save it into capitakdf structure.
This approach is the fastest one we tried and for the whole parsing,cleaning,storing to HDF buckets we need approximately 10 minutes (in my machine).

In another version, we also tried to discard the intermediate order book structure we used to hold and change all the time, and apply each message from
the exchange directly to capitakdf. Although it may seem logical that this approach should be faster 
(as we don't copy the data from the intermediate order book to the data structure each time) it turns out to be much slower.
The reason is probably because the data structure is too large and it takes more time to perform these operations in its whole than
to keep a small instance that we are altering all the time. 
This approach takes around 20 minutes.

Of course we adopt the second one...


Execution details
-------------------
As in every script typing -h will give you help in the execution procedure.
But let's see what you need in order to run this script:
	- One folder with all the raw .csv files inside.
	- Optionally you can create yourself the output directory where the HDF buckets will be stored, but if you are bored, just specify the path and the script will create it for you.

[Basic]
python _1_preallocate_readCSVandFix_Class.py -i [path of the directory where your csv lie] -o [path of the directory to store the HDF buckets--don't forget to put a / in the end of the path]

[Optional]
python _1_preallocate_readCSVandFix_Class.py -i [path of the directory where your csv lie] -o [path of the directory to store the HDF buckets] -d [depth of the order books]
(I have disabled test mode so you don't need to worry about the -t and -tl options)