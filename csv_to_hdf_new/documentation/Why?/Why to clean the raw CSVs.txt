The main reason why we want to clean and reproduce all of the raw .csv files is because we noticed some inconsistencies in the data. 
This also means that the HDF files created by Alex contain wrong values as well.

Let's see some of the bugs we refer to:

1) Wrong quote prices in the order books.
Although the messages from the exchange have the bid/ask prices with 5 decimal points,
the order book is created and printed with much fewer decimals.
This results in an order book instances that have the same, wrong price in all the depths.
For example you can see an instance here:
A,1,7100000,1.30118,7686343:415038211
A,1,500000,1.30119,7686343:415047408
OB,EUR/USD,7686343:415047408,1355299651:166000000
Q,7686343:414906936,0,1,1.3011,3000000,3000000
Q,7686343:414918951,0,2,1.3011,500000,500000
Q,7686343:414928746,0,3,1.301,500000,500000
Q,7686343:414938292,0,4,1.301,6000000,6000000
Q,7686343:414948530,0,5,1.301,1000000,1000000
Q,7686343:414994499,1,1,1.3011,1000000,1000000
Q,7686343:415017856,1,2,1.3012,3000000,3000000
Q,7686343:415028097,1,3,1.3012,3500000,3500000
Q,7686343:415038211,1,4,1.3012,7100000,7100000
Q,7686343:415047408,1,5,1.3012,500000,500000
Can be detected in several files, i.e. FXCM_EURUSD_2012_12_12.csv
We solve this issue by parsing the file, reading all the messages and re-constructing every order book from scratch.

2) Another major problem is that the format of the time stamp of several thousands of order books
in each file is wrong (the last feature of this line OB,EUR/USD,30113591:93020093,1344384045:443000000). 
Most of the times the problem is in the number of the digits of the nanosecond part of the time stamp. 
We should always have 9 digits for the nanoseconds but the variety that we have come across so far is 0,6,7,8 and 9 digits. 
The easy part to notice is when there are no digits, because you can easily assume that there are 9 zeros implied. 
The difficult part to debug is in the cases where there are fewer (i.e. 8) and you have to fill zeros. 
It is a problematic case because even if you put a zero in the beginning or in the end of the nanosecond sequence, 
you still get a valid date and you can't tell which one is correct. 
The margin between the previous and the following time stamps is large enough to allow both 
newly created time stamps to be valid.
Timir found the bug in the c++ code and the solution was to fill the preceding positions of the time stamp with zeros. 
(I have saved the bugged c++ script if you are interested..)

3) This is not a bug but still remains an important difference, especially, 
if we later on develop strategies that take into account the depth of the order book.
The CSV files we create after cleaning up the old CSVs may have fewer lines than the first ones. 
This is due to the way we chose to create the order books in memory (before we output them in the file).
We read the messages sequentially and we keep only an N-depth level order book (which is specified in the beginning 
of the program).
We can illustrate it with a simple example. Let's assume we have a 5 level depth order book that is already filled.
We get an 'A' message on the ask price from the exchange which price is higher than the one in the last depth of the order book
and therefore should not be added. The next message we get is a delete of one of the previous values.
All in all, we end up with an order book with 4 records inside.
In these cases the .csv file would have 5 records instead of 4. 
The way the order books in the .csv files must have been output is the following. As long as I am getting 'A' messages from
the exchange, keep a list of records and keep increasing it, no matter what is the depth.
Then when its the time to print the order book, just throw out everything that is from the maximum depth and below.
In our case though, we are storing a fixed position array because its much faster than having a list that needs to be converted 
to a numpy array all the time.One solution would be to assume that we are not going to receive more than 10 addition messages
in one run, so we can keep a maximum level of 10 positions.
Unfortunately no one can guarantee this.
For example, the AUDCAD pair at August 8 2012, has 33 lines of consecutive 'A' messages. Therefore, we can't assume that
we are only getting a fixed size of messages every time the exchange sends information.
Thus we chose to stick with the implementation that may discard some levels of the order book (something which at least I think is correct
as the messages are processed sequentially and we don't do anything weird when reading them).
Moreover, at this point, we are not having any strategies that take into account the depths and the volumes from multiple levels
so this minor detail should not affect us so much. If in the future this becomes an issue, we will be called to solve it...

4) In the process of cleaning up the files from the above bugs, we also deal with the RESTART messages. 
We read the messages from the exchange into a buffer and if we notice a RESTART message,
we simply discard the whole chunk and keep on reading the following messages, 
without changing or outputting the state of the order book.
In this way we don't have to back roll the order book and discard any changes we might have made, in a case of a restart.
We don't assume that the restart will happen in a specific position of each line so we can catch it wherever it may occur.

5) There other type of anomalies that can occur such as:
- There may be some half finished line, which omits for example the volume of the quote to be added in the order book.
- In worst cases there might be a line which doesn't omit it but has nothing inside. 
  So it returns a "" which is really annoying as it seems to be okay but it truly isn't.
- Moreover there might be some half finished order books that really don't make any sense, like:
	OB,EUR/USD,30199941:326848732,1344470396:882000000
	Q,30199938:474323527,0,1,1.23715,1000000,1000000
	Q,30199940:512208791,0,2,1.23713,3000000,3000000
	Q,30199940:512227507,0,3,1.23712,3000000,3000000
	Q,30199941:326830225,0,4,1.23711,4000000,4000000
	Q,30199941:326848732,0,5,1.2371,8600000,8600000
	Q,30199931
Nevertheless, we deal with all those types of inconsistencies

