A very logical question to be asked is why on earth we want to go through the same hell that 
Alex Rubinsteyn went through in order to create the HDF buckets we used to store the data and do
this whole magnificent analysis and feature extraction that they contain.

1st) and most important.. There is no guaranteed data alignment in the files.
If you go through the old code you will understand my point but a simple experiment will convince you.
Just go and change the first time stamp of an order book in the csv and then run the script to create the buckets.
You will see that the feature 't' inside the HDF is shifted by how severe was the impact of our change in the time stamp.
In other words, if for some reason, the exchange does not send data in time for a specific pair, the whole bucket will be 
totally unaligned with the other ones.
 
If we want to further elaborate on this, there is another logical argument which proves that this procedure can't
guarantee aligned time series.
If you have to align files 1, 2 and 3, you first of all need to clarify which time frames have in common and which are unique
for every bucket (that's what we do in the 2_alignReindexThemWithAllThedates script).
This means that the program must examine all the three files before it outputs the bucket for each one of them.
 
In contrast, when forming buckets using run_extractor.py script you only need to specify the path of the currency pair
you want to creates the bucket for. 
This way you don't have any knowledge on what is happening with the other pairs, if they start later/earlier and if you should
change the starting point of this bucket to be earlier/later.


2nd) We don't really know what would be the best time aggregation for the data. The older code is very sophisticated and although
I have read only a small portion of it, you can't miss the cleverness of its flow.
Nonetheless, it is not easy (at least for me), to interfere into the code and easily group the data in various time frames.
In 4_aggregation.py script you will find a very easy way to alter the grouping of your data in whatever time frame you may
wish, only by changing one line of code!

3rd) All the other issues about performance and ease of use are only supplementary on why we want a data structure
that will be robust, easy to maintain and easy to build upon. 